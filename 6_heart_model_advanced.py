# ğŸ“¦ Temel kÃ¼tÃ¼phaneler
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import time
from datetime import datetime

# ğŸ“Š Makine Ã¶ÄŸrenmesi araÃ§larÄ±
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    confusion_matrix, classification_report,
    roc_auc_score, roc_curve, accuracy_score, precision_score, recall_score, f1_score
)
from sklearn.inspection import permutation_importance
from sklearn.ensemble import RandomForestClassifier

# ğŸ” Veri dengeleme
from imblearn.over_sampling import SMOTE

# ğŸ§  Yapay sinir aÄŸÄ± bileÅŸenleri
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

# ğŸ” AÃ§Ä±klanabilir yapay zeka (XAI)


# 1. Veriyi yÃ¼kle
heart_df = pd.read_csv("heart.csv")
X = heart_df.drop("target", axis=1)
y = heart_df["target"]

# 2. SÄ±nÄ±f daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶ster
print("ğŸ” SÄ±nÄ±f DaÄŸÄ±lÄ±mÄ±:\n", y.value_counts())

# 3. SMOTE ile dengeleme
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)
print("âœ… SMOTE sonrasÄ± daÄŸÄ±lÄ±m:\n", pd.Series(y_resampled).value_counts())

# 4. Normalize et
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_resampled)

# 5. EÄŸitim-test bÃ¶lmesi
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y_resampled, test_size=0.2, random_state=42
)

# 6. GeliÅŸmiÅŸ model oluÅŸtur
model = Sequential()
model.add(Dense(128, input_dim=X.shape[1]))
model.add(LeakyReLU(alpha=0.1))
model.add(BatchNormalization())
model.add(Dropout(0.4))
model.add(Dense(64))
model.add(LeakyReLU(alpha=0.1))
model.add(BatchNormalization())
model.add(Dropout(0.3))
model.add(Dense(32))
model.add(LeakyReLU(alpha=0.1))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

print("\nğŸ“ Model Ã–zeti:")
model.summary()

# 7. Callbacks
model_name = f"heart_model_advanced_{datetime.now().strftime('%Y%m%d')}.h5"
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
checkpoint = ModelCheckpoint(model_name, monitor='val_loss', save_best_only=True)

# 8. EÄŸitimi baÅŸlat
print("\nğŸš€ EÄŸitim baÅŸlÄ±yor...")
start = time.time()
history = model.fit(
    X_train, y_train,
    validation_split=0.2,
    epochs=100,
    batch_size=16,
    callbacks=[early_stop, checkpoint],
    verbose=1
)
end = time.time()
print(f"\nâ±ï¸ EÄŸitim sÃ¼resi: {end - start:.2f} saniye")

# 9. DeÄŸerlendirme
loss, acc = model.evaluate(X_test, y_test)
print(f"\nâœ… Test DoÄŸruluÄŸu: %{acc * 100:.2f}")

# 10. Tahminler
probs = model.predict(X_test).ravel()
y_pred = (probs >= 0.5).astype(int)

# 11. Ek Metrikler
print("\nğŸ” Ek Metrikler:")
print("Accuracy:  ", round(accuracy_score(y_test, y_pred), 3))
print("Precision: ", round(precision_score(y_test, y_pred), 3))
print("Recall:    ", round(recall_score(y_test, y_pred), 3))
print("F1 Score:  ", round(f1_score(y_test, y_pred), 3))
print("ROC AUC:   ", round(roc_auc_score(y_test, probs), 3))

# 12. Classification Report
print("\nğŸ§¾ Classification Report:")
print(classification_report(y_test, y_pred))

# 13. Confusion Matrix
plt.figure(figsize=(6, 4))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Purples')
plt.title("Confusion Matrix")
plt.xlabel("Tahmin")
plt.ylabel("GerÃ§ek")
plt.show()

# 14. DoÄŸruluk grafiÄŸi
plt.plot(history.history['accuracy'], label='EÄŸitim')
plt.plot(history.history['val_accuracy'], label='DoÄŸrulama')
plt.title('Model DoÄŸruluk')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid()
plt.show()

# 15. KayÄ±p grafiÄŸi
plt.plot(history.history['loss'], label='EÄŸitim')
plt.plot(history.history['val_loss'], label='DoÄŸrulama')
plt.title('Model KayÄ±p')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid()
plt.show()

# 16. ROC EÄŸrisi
fpr, tpr, _ = roc_curve(y_test, probs)
plt.plot(fpr, tpr, label=f"AUC = {roc_auc_score(y_test, probs):.2f}")
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.title("ROC Curve")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.grid()
plt.show()

# 17. Ã–zellik Ã–nemleri (Permutation Importance)
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)
perm = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=42)
importances = pd.Series(perm.importances_mean, index=X.columns)
importances.sort_values().plot(kind='barh', title='Ã–zellik Ã–nem SkorlarÄ±')
plt.grid()
plt.show()

# 18. K-Fold Cross Validation
print("\nğŸ“Š K-Fold Cross Validation BaÅŸlÄ±yor...")
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_accuracies = []
cv_aucs = []

for fold, (train_idx, val_idx) in enumerate(kfold.split(X_scaled, y_resampled)):
    print(f"\nğŸ” Fold {fold+1}")
    X_tr, X_val = X_scaled[train_idx], X_scaled[val_idx]
    y_tr, y_val = y_resampled[train_idx], y_resampled[val_idx]

    fold_model = Sequential()
    fold_model.add(Dense(128, input_dim=X.shape[1]))
    fold_model.add(LeakyReLU(alpha=0.1))
    fold_model.add(BatchNormalization())
    fold_model.add(Dropout(0.4))
    fold_model.add(Dense(64))
    fold_model.add(LeakyReLU(alpha=0.1))
    fold_model.add(BatchNormalization())
    fold_model.add(Dropout(0.3))
    fold_model.add(Dense(32))
    fold_model.add(LeakyReLU(alpha=0.1))
    fold_model.add(Dropout(0.2))
    fold_model.add(Dense(1, activation='sigmoid'))
    fold_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    fold_model.fit(X_tr, y_tr, epochs=50, batch_size=16, verbose=0)
    val_probs = fold_model.predict(X_val).ravel()
    val_preds = (val_probs >= 0.5).astype(int)

    acc = accuracy_score(y_val, val_preds)
    auc = roc_auc_score(y_val, val_probs)

    print(f"âœ… Fold {fold+1} Accuracy: {acc:.3f}, AUC: {auc:.3f}")
    cv_accuracies.append(acc)
    cv_aucs.append(auc)

print("\nğŸ“ˆ K-Fold SonuÃ§larÄ±:")
print("Ortalama Accuracy: ", round(np.mean(cv_accuracies), 3))
print("Ortalama ROC AUC: ", round(np.mean(cv_aucs), 3))

# 18. SHAP DeÄŸerlendirmesi
print("\nğŸ”¬ SHAP analizine baÅŸlanÄ±yor...")

# EÄŸitim verisinden kÃ¼Ã§Ã¼k bir Ã¶rnek seÃ§
background = X_train[np.random.choice(X_train.shape[0], 100, replace=False)]

# SHAP iÃ§in DeepExplainer kullan (Keras model)
explainer = shap.DeepExplainer(model, background)
shap_values = explainer.shap_values(X_test[:100])  # Ä°lk 100 Ã¶rnek iÃ§in

# Ã–zellik isimlerini kullanmak iÃ§in DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼r
X_test_df = pd.DataFrame(X_test[:100], columns=X.columns)

# SHAP Ã¶zet grafiÄŸi
shap.summary_plot(shap_values[0], X_test_df, plot_type="bar", show=True)

# 19. Kaydet
model.save(model_name)
print(f"\nğŸ’¾ GeliÅŸmiÅŸ model kaydedildi: {model_name}")
# 20. Modelin ilk katmanÄ±na giriÅŸ iÃ§in bir Ã¶rnek verisi seÃ§ (ilk 100 Ã¶rnek alÄ±nabilir)
X_explain = X_test[:100]

# 21. SHAP iÃ§in explainer oluÅŸtur (DeepExplainer yerine GradientExplainer da denenebilir)
explainer = shap.Explainer(model, X_train)

# 22. SHAP deÄŸerlerini hesapla
print("\nğŸ” SHAP aÃ§Ä±klamalarÄ± hesaplanÄ±yor...")
shap_values = explainer(X_explain)

# 23. SHAP Ã¶zet grafiÄŸi
shap.summary_plot(shap_values, X_explain, feature_names=X.columns)